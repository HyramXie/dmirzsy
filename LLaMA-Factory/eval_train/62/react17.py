# import json
# import os
# from openai import OpenAI
# from tqdm import tqdm

# # åˆå§‹åŒ– DeepSeek API
# client = OpenAI(
#     api_key="sk-100b432f23414ba8a71a21edd60f7a99",
#     base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # æˆ–ä½ çš„ä»£ç†åœ°å€
# )

# input_file = "/root/user/LLaMA-Factory/eval_train/62/qwen2.5vl-7b/inconsistent_predictions_17.jsonl"
# output_file = "/root/user/LLaMA-Factory/eval_train/62/qwen2.5vl-7b/17_explain.jsonl"

# # è·å–å·²å¤„ç†çš„ prompt åˆ—è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘ï¼‰
# processed_prompts = set()
# if os.path.exists(output_file):
#     with open(output_file, 'r', encoding='utf-8') as f:
#         for line in f:
#             try:
#                 data = json.loads(line)
#                 processed_prompts.add(data['prompt'])
#             except:
#                 continue

# # åŠ è½½å…¨éƒ¨è¾“å…¥æ•°æ®
# with open(input_file, 'r', encoding='utf-8') as f:
#     all_data = [json.loads(line) for line in f]

# for entry in tqdm(all_data):
#     prompt = entry['prompt']
#     predict = entry['predict']
#     label = entry['label']

#     # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ ·æœ¬
#     if prompt in processed_prompts:
#         continue

#     # æ„å»ºæé—®å†…å®¹ï¼ˆæç¤ºè¯ï¼‰
#     system_prompt = "You are an expert at diagnosing and improving LLM predictions."

#     user_prompt = f"""The following is a mistaken prediction by a model:
# Text Prompt: {prompt}
# Model Prediction: {predict}
# Ground Truth Label: {label}

# Please provide:
# 1. Reflection: Where did the reasoning go wrong, and why?
# 2. Improvement: How can the model improve its reasoning to get the correct answer?

# Respond in this format:
# Reflection: ...
# Improvement: ...
# """

#     try:
#         # è°ƒç”¨ DeepSeek API
#         response = client.chat.completions.create(
#             model="deepseek-v3",  # æ›¿æ¢ä¸ºä½ å®é™…ç”¨çš„ deepseek-v3 æ¨¡å‹å
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_prompt}
#             ],
#             temperature=0.7,
#             max_tokens=256
#         )

#         reply = response.choices[0].message.content.strip()

#         # è§£ææ¨¡å‹è¾“å‡ºï¼ˆå»ºè®®æ ¼å¼æ¸…æ™°ï¼Œé˜²æ­¢å‡ºé”™ï¼‰
#         reflection = ""
#         improvement = ""
#         for line in reply.splitlines():
#             if line.startswith("Reflection:"):
#                 reflection = line.replace("Reflection:", "").strip()
#             elif line.startswith("Improvement:"):
#                 improvement = line.replace("Improvement:", "").strip()

#         # æ„é€ è¾“å‡ºç»“æ„
#         result = {
#             "prompt": prompt,
#             "predict": predict,
#             "label": label,
#             "Reflection": reflection,
#             "Improvement": improvement
#         }

#         # è¿½åŠ ä¿å­˜
#         with open(output_file, 'a', encoding='utf-8') as out_f:
#             out_f.write(json.dumps(result, ensure_ascii=False) + '\n')

#         print(f"Processed: {prompt[:50]}...")

#     except Exception as e:
#         print(f"Error processing prompt: {prompt[:50]}... Error: {e}")
#         continue

import json
import os
from openai import OpenAI
from tqdm import tqdm
import time

# åˆå§‹åŒ– DeepSeek API
client = OpenAI(
    api_key="sk-100b432f23414ba8a71a21edd60f7a99",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # æˆ–ä½ çš„ä»£ç†åœ°å€
)

input_file = "/root/user/zsy/LLaMA-Factory/eval_train/62/qwen3vl-4b/inconsistent_predictions_17.jsonl"
output_file = "/root/user/zsy/LLaMA-Factory/eval_train/62/qwen3vl-4b/17_explain.jsonl"

# è·å–å·²å¤„ç†çš„ prompt åˆ—è¡¨ï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘ï¼‰
processed_prompts = set()
if os.path.exists(output_file):
    with open(output_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                processed_prompts.add(data['prompt'])
            except:
                continue

# åŠ è½½å…¨éƒ¨è¾“å…¥æ•°æ®
with open(input_file, 'r', encoding='utf-8') as f:
    all_data = [json.loads(line) for line in f]

# ç»Ÿè®¡æŒ‡æ ‡
total_time = 0
total_prompt_tokens = 0
total_completion_tokens = 0
total_tokens = 0
processed_count = 0
error_count = 0

start_time = time.time()

for entry in tqdm(all_data, desc="Processing entries"):
    prompt = entry['prompt']
    predict = entry['predict']
    label = entry['label']

    # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ ·æœ¬
    if prompt in processed_prompts:
        continue

    # æ„å»ºæé—®å†…å®¹ï¼ˆæç¤ºè¯ï¼‰
    system_prompt = "You are an expert at diagnosing and improving LLM predictions."

    user_prompt = f"""The following is a mistaken prediction by a model:
Text Prompt: {prompt}
Model Prediction: {predict}
Ground Truth Label: {label}

Please provide:
1. Reflection: Where did the reasoning go wrong, and why?
2. Improvement: How can the model improve its reasoning to get the correct answer?

Respond in this format:
Reflection: ...
Improvement: ...
"""

    try:
        # è®°å½•å¼€å§‹æ—¶é—´
        request_start_time = time.time()
        
        # è°ƒç”¨ DeepSeek API
        response = client.chat.completions.create(
            model="deepseek-v3",  # æ›¿æ¢ä¸ºä½ å®é™…ç”¨çš„ deepseek-v3 æ¨¡å‹å
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=256
        )
        
        request_end_time = time.time()
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        request_time = request_end_time - request_start_time
        total_time += request_time

        # ç´¯è®¡tokenç»Ÿè®¡
        usage = response.usage
        total_prompt_tokens += usage.prompt_tokens if usage else 0
        total_completion_tokens += usage.completion_tokens if usage else 0
        total_tokens += (usage.prompt_tokens + usage.completion_tokens) if usage else 0

        reply = response.choices[0].message.content.strip()

        # è§£ææ¨¡å‹è¾“å‡ºï¼ˆå»ºè®®æ ¼å¼æ¸…æ™°ï¼Œé˜²æ­¢å‡ºé”™ï¼‰
        reflection = ""
        improvement = ""
        for line in reply.splitlines():
            if line.startswith("Reflection:"):
                reflection = line.replace("Reflection:", "").strip()
            elif line.startswith("Improvement:"):
                improvement = line.replace("Improvement:", "").strip()

        # æ„é€ è¾“å‡ºç»“æ„
        result = {
            "prompt": prompt,
            "predict": predict,
            "label": label,
            "Reflection": reflection,
            "Improvement": improvement
        }

        # è¿½åŠ ä¿å­˜
        with open(output_file, 'a', encoding='utf-8') as out_f:
            out_f.write(json.dumps(result, ensure_ascii=False) + '\n')

        processed_count += 1
        print(f"Processed: {prompt[:50]}...")

    except Exception as e:
        error_count += 1
        print(f"Error processing prompt: {prompt[:50]}... Error: {e}")
        continue

end_time = time.time()
overall_time = end_time - start_time

# è®¡ç®—æœªå¤„ç†çš„åŸå§‹æ•°æ®é‡
total_input_count = len(all_data)
skipped_count = total_input_count - processed_count - error_count

# è¾“å‡ºæ•ˆç‡æŒ‡æ ‡
print(f"\nğŸ“Š Processing Summary:")
print(f"ğŸ“ Total input entries: {total_input_count}")
print(f"â­ï¸  Skipped (already processed): {skipped_count}")
print(f"âœ… Successfully processed: {processed_count}")
print(f"âŒ Errors occurred: {error_count}")
print(f"ğŸ¯ Actually processed in this run: {processed_count}")
print(f"â±ï¸  Total processing time: {total_time:.2f}s")
print(f"â±ï¸  Overall time (including setup): {overall_time:.2f}s")
if processed_count > 0:
    print(f"âš¡ Average time per entry: {total_time/processed_count:.2f}s")
    print(f"ğŸ“ˆ Average prompt tokens per entry: {total_prompt_tokens/processed_count:.2f}")
    print(f"ğŸ“ˆ Average completion tokens per entry: {total_completion_tokens/processed_count:.2f}")
    print(f"ğŸ“ˆ Average total tokens per entry: {total_tokens/processed_count:.2f}")
print(f"ğŸ“ Total prompt tokens: {total_prompt_tokens}")
print(f"ğŸ“ Total completion tokens: {total_completion_tokens}")
print(f"ğŸ“ Total tokens: {total_tokens}")
print(f"âœ… Results saved to: {output_file}")