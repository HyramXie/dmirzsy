import os
import json
import gc
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoProcessor, AutoModelForVision2Seq

# é˜²ç¢ç‰‡è®¾ç½®
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# æ¨¡å‹è·¯å¾„ï¼ˆç¡®ä¿å’Œä¹‹å‰ä¸€è‡´ï¼‰
model_path = "/public/home/byxu_jsjxy/ywl/pretrained/google/gemma-3-27b-it"

# åŠ è½½ tokenizer å’Œ processor
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

# åŠ è½½æ¨¡å‹ï¼ˆä¿æŒ float16 å’Œ device_mapï¼‰
model = AutoModelForVision2Seq.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
model.eval()

# è¾“å…¥æ–‡ä»¶ï¼šä¹‹å‰æå–å‡ºçš„ä¸ä¸€è‡´æ ·æœ¬
inconsistent_file = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/2-inconsistent_predictions.json"
output_reflect_path = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/3-reflected_judgments.json"

# æ¨ç†å‚æ•°
max_new_tokens = 128
num_return_sequences = 5  # å†ç”Ÿæˆ5æ¡åæ€å›ç­”
reflected_results = []

# åŠ è½½ä¸ä¸€è‡´æ ·æœ¬
with open(inconsistent_file, "r", encoding="utf-8") as f:
    inconsistent_samples = json.load(f)

print("ğŸ”„ å¼€å§‹è¿›è¡Œåæ€å¼æ¨ç†ï¼ˆReflection over inconsistent predictionsï¼‰...")

for item in tqdm(inconsistent_samples):
    try:
        original_data = item["original"]
        image_path = original_data["images"][0]
        image = Image.open(image_path).convert("RGB")

        # æå–åŸå§‹ user å†…å®¹
        user_msg = next(msg for msg in original_data["messages"] if msg["role"] == "user")
        user_content = user_msg["content"]

        # æå–ç¬¬ä¸€è½®çš„5ä¸ª model_outputsï¼ˆå·²æœ‰çš„ç”Ÿæˆç»“æœï¼‰
        first_round_outputs = original_data.get("model_outputs", [])
        votes = item["extracted_sentiments"]  # å¦‚ ["Negative", "Neutral", ...]

        # æ„å»ºåæ€ prompt
        reflection_prompt = (
            f"Original task:\n{user_content}\n\n"
            f"Based on the image, text, and description above, "
            f"you previously generated 5 responses with mixed sentiment judgments:\n"
        )
        for i, (out, vote) in enumerate(zip(first_round_outputs, votes)):
            reflection_prompt += f"{i+1}. {out} ({vote})\n"
        
        reflection_prompt += (
            "\nNow, please re-evaluate all these opinions and provide a final judgment.\n"
            "Answer in exactly this format: \"Final Sentiment: [Positive/Neutral/Negative]. Reason: [brief explanation].\"\n"
            "Be concise and focus on evidence from the image, text, and reasoning."
        )

        # æ„é€ å¯¹è¯
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": reflection_prompt}
                ]
            }
        ]

        # ä½¿ç”¨ tokenizer æ„å»ºæ¨¡æ¿
        full_prompt = tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )

        # å¤„ç†è¾“å…¥
        inputs = processor(text=full_prompt, images=[image], return_tensors="pt").to(model.device, torch.float16)

        # ç”Ÿæˆå¤šæ¡åæ€å›ç­”
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                num_return_sequences=num_return_sequences,
                pad_token_id=tokenizer.eos_token_id
            )

        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        # æ¸…æ´—è¾“å‡ºï¼šæå– assistant å›å¤
        reflected_responses = []
        for output in decoded_outputs:
            if "assistant" in output:
                response = output.split("assistant")[-1].strip()
            else:
                response = output.strip()

            # ç®€å•æ¸…æ´—æ ¼å¼
            if not response.startswith("Final Sentiment:"):
                response = "Final Sentiment: " + response  # å°½é‡è¡¥å…¨
            reflected_responses.append(response)

        # ä¿å­˜ç»“æœï¼ˆä¿ç•™åŸå§‹åˆ†æ­§ä¿¡æ¯ + æ–°å¢åæ€è¾“å‡ºï¼‰
        reflected_results.append({
            "original_inconsistent_sample": original_data,
            "first_round_votes": votes,
            "first_round_outputs": first_round_outputs,
            "distribution": item["distribution"],
            "reflected_outputs": reflected_responses  # ç¬¬äºŒè½®åæ€ç»“æœ
        })

    except Exception as e:
        print(f"Error during reflection for {original_data['images'][0]}: {e}")
        reflected_results.append({
            "error": str(e),
            "image": original_data.get("images", ["unknown"])[0]
        })
    finally:
        torch.cuda.empty_cache()
        gc.collect()

# ä¿å­˜åæ€ç»“æœ
with open(output_reflect_path, "w", encoding="utf-8") as f:
    json.dump(reflected_results, f, indent=2, ensure_ascii=False)

print(f"âœ… åæ€å®Œæˆï¼å…±å¤„ç† {len(reflected_results)} ä¸ªä¸ä¸€è‡´æ ·æœ¬ã€‚")
print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {output_reflect_path}")