import os
import json
import gc
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoProcessor, AutoModelForVision2Seq

# é˜²ç¢ç‰‡è®¾ç½®
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# æ¨¡å‹è·¯å¾„
model_path = "/public/home/byxu_jsjxy/ywl/pretrained/Qwen/Qwen2.5-VL-32B-Instruct"

# åŠ è½½ç»„ä»¶
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

model = AutoModelForVision2Seq.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
model.eval()

# è¾“å…¥æ–‡ä»¶ï¼šç¬¬äºŒè½®åæ€åä»ä¸ä¸€è‡´çš„æ ·æœ¬
input_file = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/4-reflected_inconsistent_again.json"
# è¾“å‡ºæ–‡ä»¶ï¼šç¬¬ä¸‰è½®æœ€ç»ˆåæ€ç»“æœ
output_file = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/5-final_reflection_round.json"

# æ¨ç†å‚æ•°
max_new_tokens = 128
num_return_sequences = 5
final_results = []

# åŠ è½½æ•°æ®
with open(input_file, "r", encoding="utf-8") as f:
    samples = json.load(f)

print("ğŸ§  å¼€å§‹ç¬¬ä¸‰è½®å…ƒåæ€ï¼ˆMeta-Reflection over second-round judgmentsï¼‰...")

for item in tqdm(samples):
    try:
        # åŸå§‹ä¿¡æ¯
        original_sample = item["original_inconsistent_sample"]
        image_path = original_sample["images"][0]
        image = Image.open(image_path).convert("RGB")

        # åŸå§‹ç”¨æˆ·é—®é¢˜
        user_msg = next(msg for msg in original_sample["messages"] if msg["role"] == "user")
        user_content = user_msg["content"]

        # ç¬¬äºŒè½®åæ€è¾“å‡ºï¼ˆå³æ¨¡å‹è‡ªå·±ä¹‹å‰çš„â€œåæ€â€ï¼‰
        previous_reflections = item.get("reflected_outputs", [])

        # æ„é€ ç¬¬ä¸‰è½® prompt
        meta_prompt = (
            f"### Task Recap:\n{user_content}\n\n"
            f"### Previous Round of Self-Reflection:\n"
            "In your own previous reflection, you generated the following 5 analyses:\n"
        )
        for i, out in enumerate(previous_reflections):
            meta_prompt += f"{i+1}. {out}\n"
        
        meta_prompt += (
            "\n### Instruction:\n"
            "Now, act as a meta-analyst. Review all 5 of your prior reflection responses critically.\n"
            "Identify patterns, contradictions, and strongest evidence.\n"
            "Then provide a final, well-reasoned sentiment judgment.\n\n"
            "Answer in this format:\n"
            "\"Final Decision: [Positive/Neutral/Negative]. Rationale: [Concise, evidence-based explanation].\"\n"
            "Do NOT just repeat one of the above. Synthesize and evaluate them."
        )

        # æ„å»ºå¯¹è¯
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": meta_prompt}
                ]
            }
        ]

        # åº”ç”¨ chat template
        full_prompt = tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True
        )

        # å¤„ç†è¾“å…¥
        inputs = processor(text=full_prompt, images=[image], return_tensors="pt").to(model.device, torch.float16)

        # ç”Ÿæˆ 5 æ¡æœ€ç»ˆç»¼åˆåˆ¤æ–­
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
                num_return_sequences=num_return_sequences,
                pad_token_id=tokenizer.eos_token_id
            )

        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)

        # æ¸…æ´—è¾“å‡º
        final_outputs = []
        for output in decoded:
            if "assistant" in output:
                response = output.split("assistant")[-1].strip()
            else:
                response = output.strip()
            final_outputs.append(response)

        # ä¿å­˜ç»“æœ
        final_results.append({
            "original_inconsistent_sample": original_sample,
            "first_round_votes": item["first_round_votes"],
            "first_round_outputs": item["first_round_outputs"],
            "reflected_outputs": previous_reflections,
            "reflected_distribution": item["reflected_distribution"],
            "final_meta_reflection": final_outputs  # ç¬¬ä¸‰è½®æœ€ç»ˆè¾“å‡º
        })

    except Exception as e:
        print(f"Error in meta-reflection for {original_sample['images'][0]}: {e}")
        final_results.append({
            "error": str(e),
            "image": original_sample["images"][0]
        })
    finally:
        torch.cuda.empty_cache()
        gc.collect()

# ä¿å­˜æœ€ç»ˆç»“æœ
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(final_results, f, indent=2, ensure_ascii=False)

print(f"âœ… ç¬¬ä¸‰è½®å…ƒåæ€å®Œæˆï¼å…±å¤„ç† {len(final_results)} ä¸ªé«˜äº‰è®®æ ·æœ¬ã€‚")
print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {output_file}")