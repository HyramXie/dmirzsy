# import json
# import re
# from tqdm import tqdm

# # è¾“å…¥æ–‡ä»¶ï¼šç¬¬äºŒè½®åæ€çš„å®Œæ•´è¾“å…¥ï¼ˆå³ inconsistent_predictions.jsonï¼‰
# input_file = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/2-inconsistent_predictions.json"
# # è¾“å‡ºæ–‡ä»¶ï¼šåŒ…å«æ¯æ¡çš„çŠ¶æ€åˆ†æ
# diagnosis_output = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/reflection_diagnosis.json"

# # åæ€ç»“æœæ–‡ä»¶ï¼ˆç¬¬äºŒè½®è¾“å‡ºï¼‰
# reflected_file = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/3-reflected_judgments.json"

# # æ”¯æŒçš„æƒ…æ„Ÿæ ‡ç­¾
# SENTIMENTS = {"Positive", "Neutral", "Negative"}

# def extract_sentiment_from_reflected(text):
#     match = re.search(r"Final\s+Sentiment\s*:\s*([a-zA-Z]+)", text, re.IGNORECASE)
#     if match:
#         word = match.group(1).capitalize()
#         if word in SENTIMENTS:
#             return word
#     return None

# # åŠ è½½åŸå§‹ä¸ä¸€è‡´æ ·æœ¬ï¼ˆåº”ä¸º 157 æ¡ï¼‰
# with open(input_file, "r", encoding="utf-8") as f:
#     original_inconsistent = json.load(f)
# print(f"ğŸ“ åŸå§‹ä¸ä¸€è‡´æ ·æœ¬æ€»æ•°: {len(original_inconsistent)}")

# # åŠ è½½åæ€åç»“æœï¼ˆå¯èƒ½å°‘äº 157ï¼‰
# try:
#     with open(reflected_file, "r", encoding="utf-8") as f:
#         reflected_results = json.load(f)
#     print(f"ğŸ“ åæ€åç»“æœæ•°é‡: {len(reflected_results)}")
# except FileNotFoundError:
#     reflected_results = []
#     print("âŒ æœªæ‰¾åˆ°åæ€ç»“æœæ–‡ä»¶")

# # å»ºç«‹æ˜ å°„ï¼šç”¨ image è·¯å¾„ä½œä¸ºå”¯ä¸€ key
# reflected_dict = {}
# for item in reflected_results:
#     try:
#         img_path = item["original_inconsistent_sample"]["images"][0]
#         reflected_dict[img_path] = item
#     except:
#         continue

# # å­˜å‚¨æ¯æ¡çš„çŠ¶æ€
# diagnosis = []

# missing_count = 0
# for item in tqdm(original_inconsistent, desc="Analyzing"):
#     try:
#         img_path = item["original"]["images"][0]
#     except:
#         img_path = "unknown_image"

#     status = {
#         "image": img_path,
#         "status": None,
#         "note": ""
#     }

#     if img_path not in reflected_dict:
#         status["status"] = "MISSING"
#         status["note"] = "No reflection result generated"
#         missing_count += 1
#         diagnosis.append(status)
#         continue

#     reflected_item = reflected_dict[img_path]
#     reflected_outputs = reflected_item.get("reflected_outputs", [])

#     if len(reflected_outputs) != 5:
#         status["status"] = "INVALID_OUTPUT_COUNT"
#         status["note"] = f"Generated {len(reflected_outputs)} responses, not 5"
#         diagnosis.append(status)
#         continue

#     # æå–æƒ…æ„Ÿ
#     sentiments = [extract_sentiment_from_reflected(out) for out in reflected_outputs]
#     valid_sentiments = [s for s in sentiments if s is not None]

#     if len(valid_sentiments) != 5:
#         status["status"] = "PARSING_FAILED"
#         failed = 5 - len(valid_sentiments)
#         status["note"] = f"Failed to parse {failed}/5 sentiments"
#         diagnosis.append(status)
#         continue

#     if len(set(valid_sentiments)) == 1:
#         status["status"] = "CONSISTENT"
#         status["final_sentiment"] = valid_sentiments[0]
#     else:
#         status["status"] = "INCONSISTENT"
#         status["distribution"] = dict(zip(SENTIMENTS, [valid_sentiments.count(s) for s in SENTIMENTS]))

#     diagnosis.append(status)

# # ä¿å­˜è¯Šæ–­ç»“æœ
# with open(diagnosis_output, "w", encoding="utf-8") as f:
#     json.dump(diagnosis, f, indent=2, ensure_ascii=False)

# # ç»Ÿè®¡
# from collections import Counter
# stats = Counter(d["status"] for d in diagnosis)

# print("\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
# for k, v in stats.items():
#     print(f"  {k}: {v}")

# print(f"\nğŸ” ç¼ºå¤±çš„ 8 æ¡æ•°æ®å¾ˆå¯èƒ½æ˜¯ä»¥ä¸‹æƒ…å†µä¹‹ä¸€:")
# if "MISSING" in stats:
#     print(f"  â€¢ {stats['MISSING']} æ¡ï¼šæœªç”Ÿæˆåæ€ç»“æœï¼ˆå¯èƒ½å›  CUDA OOMã€è¶…æ—¶ã€ç¨‹åºä¸­æ–­ï¼‰")
# if "INVALID_OUTPUT_COUNT" in stats:
#     print(f"  â€¢ {stats['INVALID_OUTPUT_COUNT']} æ¡ï¼šç”Ÿæˆæ•°é‡ä¸ä¸º5")
# if "PARSING_FAILED" in stats:
#     print(f"  â€¢ {stats['PARSING_FAILED']} æ¡ï¼šæ— æ³•è§£ææƒ…æ„Ÿæ ‡ç­¾")

# print(f"\nğŸ“ è¯¦ç»†è¯Šæ–­å·²ä¿å­˜è‡³: {diagnosis_output}")


# import json
# from collections import Counter

# # å‡è®¾ä½ çš„æ•°æ®ä¿å­˜åœ¨ä¸€ä¸ªåä¸º data.json çš„æ–‡ä»¶ä¸­
# with open('/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/4-hebing.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)

# import json
# import re

# def extract_sentiment(text):
#     """ä» model_output ä¸­æå–ç¬¬ä¸€ä¸ªå‡ºç°çš„æƒ…æ„Ÿææ€§ï¼ˆPositive/Neutral/Negativeï¼‰ï¼Œå¿½ç•¥å¤§å°å†™"""
#     # æŒ‰é¡ºåºæœç´¢è¿™ä¸‰ä¸ªè¯çš„é¦–æ¬¡å‡ºç°
#     pattern = r'\b(positive|neutral|negative)\b'
#     match = re.search(pattern, text, re.IGNORECASE)
#     if match:
#         return match.group(1).lower()  # è¿”å›å°å†™å½¢å¼ä¾¿äºæ¯”è¾ƒ
#     return None  # æœªæ‰¾åˆ°æƒ…æ„Ÿè¯

# consistent_count = 0
# inconsistent_count = 0

# for item in data:
#     model_outputs = item.get("model_outputs", [])
#     if len(model_outputs) < 5:
#         # å¦‚æœ model_outputs ä¸è¶³5ä¸ªï¼Œè§†ä¸ºä¸ä¸€è‡´ï¼ˆæˆ–å¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
#         inconsistent_count += 1
#         continue

#     # æå–æ¯ä¸ª model_output çš„ç¬¬ä¸€ä¸ªæƒ…æ„Ÿææ€§
#     sentiments = []
#     for output in model_outputs:
#         sent = extract_sentiment(output)
#         sentiments.append(sent)

#     # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æå–å‡ºçš„æƒ…æ„Ÿéƒ½ç›¸åŒä¸”æœ‰æ•ˆ
#     if None in sentiments:
#         inconsistent_count += 1
#     elif len(set(sentiments)) == 1:  # æ‰€æœ‰å…ƒç´ ç›¸åŒ
#         consistent_count += 1
#     else:
#         inconsistent_count += 1

# print(f"äº”ä¸ªææ€§é¢„æµ‹ä¸€è‡´çš„æ•°æ®æ¡æ•°: {consistent_count}")
# print(f"äº”ä¸ªææ€§é¢„æµ‹ä¸ä¸€è‡´çš„æ•°æ®æ¡æ•°: {inconsistent_count}")

import json
import re

input_file = "/public/home/byxu_jsjxy/ywl/LLaMA-Factory/data/2015/4-hebing.json"

SENTIMENTS = {"Positive", "Neutral", "Negative"}

def extract_sentiment(text):
    match = re.search(r'\b(Positive|Neutral|Negative)\b', text, re.IGNORECASE)
    if match:
        return match.group(1).capitalize()  # ç»Ÿä¸€æ ‡å‡†åŒ–
    return None

# è°ƒè¯•è®¡æ•°å™¨
total_items = 0
valid_5_outputs = 0
all_5_extracted = 0
all_5_consistent = 0
four_consistent_one_none = 0
four_consistent_one_diff = 0

with open(input_file, "r", encoding="utf-8") as f:
    data = json.load(f)

print("ğŸ” å¼€å§‹è°ƒè¯•åˆ†æ...")

for item in data:
    total_items += 1
    model_outputs = item.get("model_outputs", [])
    
    if len(model_outputs) < 5:
        continue
    valid_5_outputs += 1

    # æå–æƒ…æ„Ÿ
    sentiments = [extract_sentiment(out) for out in model_outputs]
    valid_sentiments = [s for s in sentiments if s in SENTIMENTS]  # ç¡®ä¿åœ¨é›†åˆä¸­

    if len(valid_sentiments) == 5:
        all_5_extracted += 1
        if len(set(valid_sentiments)) == 1:
            all_5_consistent += 1
        else:
            # 5 ä¸ªéƒ½æœ‰æ•ˆä½†ä¸ä¸€è‡´
            pass
    elif len(valid_sentiments) == 4:
        if len(set(valid_sentiments)) == 1:
            # 4 ä¸ªä¸€è‡´ï¼Œ1 ä¸ªå¤±è´¥
            four_consistent_one_none += 1
        else:
            # 4 ä¸ªä¸­æœ‰ä¸åŒ
            four_consistent_one_diff += 1
    

    # åœ¨è°ƒè¯•å¾ªç¯ä¸­åŠ å…¥
    if len(valid_sentiments) == 4 and len(set(valid_sentiments)) == 1:
        print(f"\nğŸŸ¡ å‘ç° 4 ä¸ªä¸€è‡´ + 1 ä¸ªæå–å¤±è´¥çš„æ ·æœ¬:")
        print(f"Image: {item['images'][0]}")
        print(f"æå–ç»“æœ: {sentiments}")
        for i, out in enumerate(model_outputs):
            print(f"Output {i+1}:\n{out.strip()}")
            match = re.search(r'\b(Positive|Neutral|Negative)\b', out, re.IGNORECASE)
            print(f"  â†’ æå–: {match.group(1).capitalize() if match else 'None'}")
        print("-" * 60)

print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_items}")
print(f"âœ… model_outputs é•¿åº¦ â‰¥5 çš„æ ·æœ¬æ•°: {valid_5_outputs}")
print(f"âœ… 5 ä¸ªæƒ…æ„Ÿå…¨éƒ¨æˆåŠŸæå–çš„æ ·æœ¬æ•°: {all_5_extracted}")
print(f"âœ… 5 ä¸ªæå–æˆåŠŸä¸”æƒ…æ„Ÿä¸€è‡´çš„æ ·æœ¬æ•°: {all_5_consistent}")
print(f"ğŸŸ¡ 4 ä¸ªä¸€è‡´ + 1 ä¸ªæå–å¤±è´¥çš„æ ·æœ¬æ•°: {four_consistent_one_none}")
print(f"ğŸ”´ 4 ä¸ªä¸€è‡´ + 1 ä¸ªä¸åŒæƒ…æ„Ÿçš„æ ·æœ¬æ•°: {four_consistent_one_diff}")